æ—¢ç„¶ä½ æ˜¯åœ¨**æ²¡æœ‰å›¾å½¢ç•Œé¢çš„ Linux æœåŠ¡å™¨**ï¼ˆHeadless Serverï¼‰ä¸Šæ“ä½œï¼Œæˆ‘ä»¬å°±ä¸èƒ½ä¾èµ– LLaMA-Factory çš„ WebUI äº†ã€‚

æˆ‘ä»¬éœ€è¦ä½¿ç”¨ **å‘½ä»¤è¡Œ (CLI)** æ¨¡å¼ã€‚åˆ«æ‹…å¿ƒï¼ŒCLI å…¶å®æ›´ç¨³å®šï¼Œè€Œä¸”æ˜¾å¾—ä½ æ›´ä¸“ä¸šï¼ˆLinux è€æ‰‹éƒ½æ˜¯æ•²å‘½ä»¤çš„ï¼‰ã€‚

ä½ çš„æ˜¾å¡æ˜¯ **24G (RTX 3090/4090)**ï¼Œéå¸¸é€‚åˆè·‘ **Qwen2.5-7B** çš„ **LoRA å¾®è°ƒ**ã€‚

ä»¥ä¸‹æ˜¯å®Œæ•´çš„ä¸€æ­¥æ­¥æ“ä½œæŒ‡å—ï¼š

-----

### ç¬¬ä¸€æ­¥ï¼šç¯å¢ƒå‡†å¤‡ (LLaMA-Factory)

å¦‚æœä½ è¿˜æ²¡å®‰è£… LLaMA-Factoryï¼Œè¯·å…ˆæ‰§è¡Œè¿™äº›å‘½ä»¤ï¼š

```bash
# 1. å…‹éš†ä»£ç åº“
git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory

# 2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ (å¼ºçƒˆå»ºè®®ï¼Œé˜²æ­¢æ±¡æŸ“ç¯å¢ƒ)
conda create -n llama_factory python=3.10 -y
conda activate llama_factory

# 3. å®‰è£…ä¾èµ– (è¿™ä¸€æ­¥å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)
pip install -e .[metrics]
pip install bitsandbytes # é‡åŒ–å¾®è°ƒå¿…é¡»ç»„ä»¶
```

-----

### ç¬¬äºŒæ­¥ï¼šæ³¨å†Œä½ çš„æ•°æ®é›† (å…³é”®ï¼)

LLaMA-Factory ä¸ä¼šè‡ªå·±æ‰«ææ–‡ä»¶å¤¹ï¼Œä½ éœ€è¦å‘Šè¯‰å®ƒä½ çš„ JSON æ–‡ä»¶å«ä»€ä¹ˆã€‚

1.  **ä¸Šä¼ æ•°æ®**ï¼š
    æŠŠä½ çš„ `steam_sft_train.json` ä¸Šä¼ åˆ° `LLaMA-Factory/data/` ç›®å½•ä¸‹ã€‚

2.  **ç¼–è¾‘æ³¨å†Œè¡¨**ï¼š
    ä½¿ç”¨ `vim` æˆ– `nano` ç¼–è¾‘ `data/dataset_info.json` æ–‡ä»¶ï¼š

    ```bash
    vim data/dataset_info.json
    ```

3.  **æ·»åŠ é…ç½®**ï¼š
    åœ¨æ–‡ä»¶çš„ JSON å¯¹è±¡ä¸­ï¼Œæ·»åŠ ä¸€æ®µä½ çš„æ•°æ®é…ç½®ï¼ˆæ³¨æ„ä¸è¦ç ´å JSON æ ¼å¼ï¼Œä¸Šä¸€é¡¹åé¢è¦åŠ é€—å·ï¼‰ï¼š

    ```json
    "steam_reviews": {
      "file_name": "steam_sft_train.json",
      "columns": {
        "prompt": "instruction",
        "query": "input",
        "response": "output"
      }
    }
    ```

    *è§£é‡Šï¼šè¿™é‡Œå‘Šè¯‰æ¡†æ¶ï¼Œæˆ‘çš„æ•°æ®é›†ä»£å·å« `steam_reviews`ï¼Œå¯¹åº”çš„æ–‡ä»¶æ˜¯ `steam_sft_train.json`ï¼ŒJSON é‡Œçš„ instruction å¯¹åº” promptï¼Œinput å¯¹åº” queryï¼Œoutput å¯¹åº” responseã€‚*

-----

### ç¬¬ä¸‰æ­¥ï¼šæ„é€ è®­ç»ƒå‘½ä»¤ (The Magic Command)

è¿™æ˜¯æœ€æ ¸å¿ƒçš„ä¸€æ­¥ã€‚é’ˆå¯¹ **24G æ˜¾å­˜**ï¼Œæˆ‘ä»¬éœ€è¦å¼€å¯ **4-bit é‡åŒ– (QLoRA)** æ¥èŠ‚çœæ˜¾å­˜ï¼ŒåŒæ—¶æŠŠ Batch Size è®¾å¤§ä¸€ç‚¹ä»¥ä¿è¯é€Ÿåº¦ã€‚

è¯·ç›´æ¥åœ¨ `LLaMA-Factory` ç›®å½•ä¸‹è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼ˆå»ºè®®å¤åˆ¶ä¿å­˜ä¸ºä¸€ä¸ª `run_train.sh` è„šæœ¬è¿è¡Œï¼‰ï¼š

```bash
# ç¡®ä¿ä½ åœ¨ LLaMA-Factory ç›®å½•ä¸‹
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train \
    --stage sft \
    --do_train \
    --model_name_or_path Qwen/Qwen2.5-7B-Instruct \
    --dataset steam_reviews \
    --dataset_dir data \
    --template qwen \
    --finetuning_type lora \
    --lora_target all \
    --output_dir saves/qwen2.5-7b-steam-lora \
    --overwrite_output_dir \
    --per_device_train_batch_size 4 \
    --gradient_accumulation_steps 4 \
    --learning_rate 2e-4 \
    --num_train_epochs 3.0 \
    --logging_steps 10 \
    --save_steps 100 \
    --warmup_ratio 0.1 \
    --fp16 True \
    --quantization_bit 4
```

#### ğŸ› ï¸ å‚æ•°è¯¦è§£ï¼ˆç­”è¾©æ—¶å¯ä»¥ç”¨ï¼‰ï¼š

  * `--model_name_or_path`: ä½¿ç”¨ Qwen2.5-7B-Instruct ä½œä¸ºåŸºåº§ï¼ˆå¦‚æœä½ è¿˜æ²¡ä¸‹è½½ï¼Œå®ƒä¼šè‡ªåŠ¨ä» HuggingFace/ModelScope ä¸‹è½½ï¼‰ã€‚
  * `--dataset steam_reviews`: åˆšæ‰æ³¨å†Œçš„æ•°æ®é›†ä»£å·ã€‚
  * `--lora_target all`: å¯¹æ¨¡å‹çš„æ‰€æœ‰å±‚è¿›è¡Œå¾®è°ƒï¼Œæ•ˆæœæ¯”é»˜è®¤å¥½ï¼Œè™½ç„¶æ…¢ä¸€ç‚¹ç‚¹ã€‚
  * `--quantization_bit 4`: **æ ¸å¿ƒï¼** 4bit é‡åŒ–åŠ è½½ã€‚è¿™æ ·æ¨¡å‹åªå  6GB æ˜¾å­˜ï¼Œå‰©ä¸‹çš„ 18GB éƒ½å¯ä»¥ç”¨æ¥æ”¾è®­ç»ƒæ•°æ®ï¼Œé˜²æ­¢ OOM (Out of Memory)ã€‚
  * `--per_device_train_batch_size 4` \* `--gradient_accumulation_steps 4`: å®é™… Batch Size = 16ã€‚è¿™æ˜¯ä¸€ä¸ªæ¯”è¾ƒç¨³å¥çš„æ•°å€¼ã€‚
  * `--learning_rate 2e-4`: LoRA å¾®è°ƒçš„æ ‡å‡†å­¦ä¹ ç‡ã€‚

-----

### ç¬¬å››æ­¥ï¼šç›‘æ§è®­ç»ƒè¿‡ç¨‹

å½“ä½ è¿è¡Œä¸Šè¿°å‘½ä»¤åï¼Œç»ˆç«¯ä¼šå¼€å§‹ç–¯ç‹‚è·³åŠ¨ Loss å€¼ã€‚

1.  **çœ‹ Loss**ï¼š
    ä½ åº”è¯¥çœ‹åˆ° `loss` ä»ä¸€å¼€å§‹çš„ `2.x` æˆ– `1.x` æ…¢æ…¢ä¸‹é™ã€‚

      * å¦‚æœé™åˆ° `0.8` ä»¥ä¸‹ï¼Œè¯´æ˜å®ƒå­¦ä¼šè¯´è¯äº†ã€‚
      * å¦‚æœé™åˆ° `0.1` ä»¥ä¸‹ï¼Œå¯èƒ½è¿‡æ‹Ÿåˆäº†ï¼ˆä¸è¿‡å¯¹äºé£æ ¼è¿ç§»ï¼Œè¿‡æ‹Ÿåˆä¸€ç‚¹æ²¡å…³ç³»ï¼‰ã€‚

2.  **é˜²æ­¢æ–­è¿**ï¼š
    å¦‚æœä½ æ˜¯ç”¨ SSH è¿çš„æœåŠ¡å™¨ï¼Œå»ºè®®ç”¨ `tmux` æˆ–è€… `nohup` è¿è¡Œï¼Œå¦åˆ™ç½‘ä¸€æ–­è®­ç»ƒå°±åœäº†ã€‚

    ```bash
    # åå°è¿è¡Œç¤ºä¾‹
    nohup bash run_train.sh > train.log 2>&1 &
    # æŸ¥çœ‹æ—¥å¿—
    tail -f train.log
    ```

-----

### ç¬¬äº”æ­¥ï¼šåœ¨ç»ˆç«¯é‡Œæµ‹è¯•æ¨¡å‹ (Inference)

è®­ç»ƒå¤§æ¦‚éœ€è¦ 1-3 å°æ—¶ï¼ˆå–å†³äºä½ çš„æ•°æ®é‡ï¼‰ã€‚è®­ç»ƒå®Œåï¼Œä¸éœ€è¦å¯¼å‡ºæ¨¡å‹ï¼Œå¯ä»¥ç›´æ¥åŠ è½½ LoRA æƒé‡åœ¨ç»ˆç«¯é‡Œæµ‹è¯•ï¼

ä½¿ç”¨ `llamafactory-cli chat` å‘½ä»¤ï¼š

```bash
CUDA_VISIBLE_DEVICES=0 llamafactory-cli chat \
    --model_name_or_path Qwen/Qwen2.5-7B-Instruct \
    --adapter_name_or_path saves/qwen2.5-7b-steam-lora \
    --template qwen \
    --quantization_bit 4
```

**è¿è¡Œåï¼Œä½ ä¼šè¿›å…¥ä¸€ä¸ªåƒ Python input çš„ç•Œé¢ï¼š**

  * **User**: è¯·ä»¥èµ„æ·±ç©å®¶çš„èº«ä»½ï¼Œç‚¹è¯„ä¸€ä¸‹ã€Šé»‘ç¥è¯ï¼šæ‚Ÿç©ºã€‹è¿™æ¬¾æ¸¸æˆã€‚
  * **Assistant**: (è¿™é‡Œæ¨¡å‹åº”è¯¥ä¼šè¾“å‡ºä½ è®­ç»ƒçš„é‚£äº›éªšè¯ï¼Œæ¯”å¦‚â€œæ‰“å‡»æ„Ÿçˆ†æ£šâ€ã€â€œç©ºæ°”å¢™æ¶å¿ƒâ€ä¹‹ç±»çš„å†…å®¹)

-----

### ğŸ’¡ ç»™ä½ çš„ Dify éƒ¨ç½²å»ºè®® (API æœåŠ¡)

ç­‰ä½ åœ¨ç»ˆç«¯é‡Œæµ‹è¯•æ»¡æ„äº†ï¼Œä½ éœ€è¦æŠŠå®ƒå˜æˆä¸€ä¸ª API ç»™ Dify è°ƒç”¨ã€‚
LLaMA-Factory è‡ªå¸¦äº† OpenAI æ ¼å¼çš„ API æœåŠ¡ï¼

```bash
CUDA_VISIBLE_DEVICES=0 llamafactory-cli api \
    --model_name_or_path Qwen/Qwen2.5-7B-Instruct \
    --adapter_name_or_path saves/qwen2.5-7b-steam-lora \
    --template qwen \
    --quantization_bit 4 \
    --api_port 8000
```

è¿è¡Œè¿™ä¸ªå‘½ä»¤åï¼Œä½ çš„æœåŠ¡å™¨ `8000` ç«¯å£å°±å¼€å¯äº†ä¸€ä¸ªç±»ä¼¼ OpenAI çš„æ¥å£ã€‚
åœ¨ Dify é‡Œé…ç½®ï¼š

  * **æ¨¡å‹æä¾›å•†**: OpenAI-API-Compatible
  * **API URL**: `http://ä½ çš„æœåŠ¡å™¨IP:8000/v1`
  * **API Key**: (éšä¾¿å¡«ï¼Œä¾‹å¦‚ `sk-123`)

è¿™æ ·ï¼Œä½ çš„ Dify æ™ºèƒ½ä½“å°±èƒ½ç”¨ä¸Šè¿™ä¸ª\*\*â€œæ»¡å˜´éªšè¯â€çš„è€ç©å®¶æ¨¡å‹\*\*äº†ï¼ğŸ®



export HF_ENDPOINT=https://hf-mirror.com

huggingface-cli download --resume-download Qwen/Qwen2.5-7B-Instruct --local-dir ./hf_cache/Qwen2.5-7B-Instruct

